{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.5.\n",
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "import pytz\n",
    "import random\n",
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from ogb.nodeproppred import Evaluator\n",
    "from dgl import function as fn\n",
    "\n",
    "CPF_data = [\"cora\", \"citeseer\", \"pubmed\", \"a-computer\", \"a-photo\"]\n",
    "OGB_data = [\"ogbn-arxiv\", \"ogbn-products\"]\n",
    "NonHom_data = [\"pokec\", \"penn94\"]\n",
    "BGNN_data = [\"house_class\", \"vk_class\"]\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def get_training_config(config_path, model_name, dataset):\n",
    "    with open(config_path, \"r\") as conf:\n",
    "        full_config = yaml.load(conf, Loader=yaml.FullLoader)\n",
    "    dataset_specific_config = full_config[\"global\"]\n",
    "    model_specific_config = full_config[dataset][model_name]\n",
    "\n",
    "    if model_specific_config is not None:\n",
    "        specific_config = dict(dataset_specific_config, **model_specific_config)\n",
    "    else:\n",
    "        specific_config = dataset_specific_config\n",
    "\n",
    "    specific_config[\"model_name\"] = model_name\n",
    "    return specific_config\n",
    "\n",
    "\n",
    "def check_writable(path, overwrite=True):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    elif overwrite:\n",
    "        shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "def check_readable(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError(f\"No such file or directory! {path}\")\n",
    "\n",
    "\n",
    "def timetz(*args):\n",
    "    tz = pytz.timezone(\"US/Pacific\")\n",
    "    return datetime.now(tz).timetuple()\n",
    "\n",
    "\n",
    "def get_logger(filename, console_log=False, log_level=logging.INFO):\n",
    "    tz = pytz.timezone(\"US/Pacific\")\n",
    "    log_time = datetime.now(tz).strftime(\"%b%d_%H_%M_%S\")\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.propagate = False  # avoid duplicate logging\n",
    "    logger.setLevel(log_level)\n",
    "\n",
    "    # Clean logger first to avoid duplicated handlers\n",
    "    for hdlr in logger.handlers[:]:\n",
    "        logger.removeHandler(hdlr)\n",
    "\n",
    "    file_handler = logging.FileHandler(filename)\n",
    "    formatter = logging.Formatter(\"%(asctime)s: %(message)s\", datefmt=\"%b%d %H-%M-%S\")\n",
    "    formatter.converter = timetz\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    if console_log:\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def idx_split(idx, ratio, seed=0):\n",
    "    \"\"\"\n",
    "    randomly split idx into two portions with ratio% elements and (1 - ratio)% elements\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    n = len(idx)\n",
    "    cut = int(n * ratio)\n",
    "    idx_idx_shuffle = torch.randperm(n)\n",
    "\n",
    "    idx1_idx, idx2_idx = idx_idx_shuffle[:cut], idx_idx_shuffle[cut:]\n",
    "    idx1, idx2 = idx[idx1_idx], idx[idx2_idx]\n",
    "    # assert((torch.cat([idx1, idx2]).sort()[0] == idx.sort()[0]).all())\n",
    "    return idx1, idx2\n",
    "\n",
    "\n",
    "def graph_split(idx_train, idx_val, idx_test, rate, seed):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        The original setting was transductive. Full graph is observed, and idx_train takes up a small portion.\n",
    "        Split the graph by further divide idx_test into [idx_test_tran, idx_test_ind].\n",
    "        rate = idx_test_ind : idx_test (how much test to hide for the inductive evaluation)\n",
    "\n",
    "        Ex. Ogbn-products\n",
    "        loaded     : train : val : test = 8 : 2 : 90, rate = 0.2\n",
    "        after split: train : val : test_tran : test_ind = 8 : 2 : 72 : 18\n",
    "\n",
    "    Return:\n",
    "        Indices start with 'obs_' correspond to the node indices within the observed subgraph,\n",
    "        where as indices start directly with 'idx_' correspond to the node indices in the original graph\n",
    "    \"\"\"\n",
    "    idx_test_ind, idx_test_tran = idx_split(idx_test, rate, seed)\n",
    "\n",
    "    idx_obs = torch.cat([idx_train, idx_val, idx_test_tran])\n",
    "    N1, N2 = idx_train.shape[0], idx_val.shape[0]\n",
    "    obs_idx_all = torch.arange(idx_obs.shape[0])\n",
    "    obs_idx_train = obs_idx_all[:N1]\n",
    "    obs_idx_val = obs_idx_all[N1 : N1 + N2]\n",
    "    obs_idx_test = obs_idx_all[N1 + N2 :]\n",
    "\n",
    "    return obs_idx_train, obs_idx_val, obs_idx_test, idx_obs, idx_test_ind\n",
    "\n",
    "\n",
    "def get_evaluator(dataset):\n",
    "    if dataset in CPF_data + NonHom_data + BGNN_data:\n",
    "\n",
    "        def evaluator(out, labels):\n",
    "            pred = out.argmax(1)\n",
    "            return pred.eq(labels).float().mean().item()\n",
    "\n",
    "    elif dataset in OGB_data:\n",
    "        ogb_evaluator = Evaluator(dataset)\n",
    "\n",
    "        def evaluator(out, labels):\n",
    "            pred = out.argmax(1, keepdim=True)\n",
    "            input_dict = {\"y_true\": labels.unsqueeze(1), \"y_pred\": pred}\n",
    "            return ogb_evaluator.eval(input_dict)[\"acc\"]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset\")\n",
    "\n",
    "    return evaluator\n",
    "\n",
    "\n",
    "def get_evaluator(dataset):\n",
    "    def evaluator(out, labels):\n",
    "        pred = out.argmax(1)\n",
    "        return pred.eq(labels).float().mean().item()\n",
    "\n",
    "    return evaluator\n",
    "\n",
    "\n",
    "def compute_min_cut_loss(g, out):\n",
    "    out = out.to(\"cpu\")\n",
    "    S = out.exp()\n",
    "    A = g.adj().to_dense()\n",
    "    D = g.in_degrees().float().diag()\n",
    "    min_cut = (\n",
    "        torch.matmul(torch.matmul(S.transpose(1, 0), A), S).trace()\n",
    "        / torch.matmul(torch.matmul(S.transpose(1, 0), D), S).trace()\n",
    "    )\n",
    "    return min_cut.item()\n",
    "\n",
    "\n",
    "def feature_prop(feats, g, k):\n",
    "    \"\"\"\n",
    "    Augment node feature by propagating the node features within k-hop neighborhood.\n",
    "    The propagation is done in the SGC fashion, i.e. hop by hop and symmetrically normalized by node degrees.\n",
    "    \"\"\"\n",
    "    assert feats.shape[0] == g.num_nodes()\n",
    "\n",
    "    degs = g.in_degrees().float().clamp(min=1)\n",
    "    norm = torch.pow(degs, -0.5).unsqueeze(1)\n",
    "\n",
    "    # compute (D^-1/2 A D^-1/2)^k X\n",
    "    for _ in range(k):\n",
    "        feats = feats * norm\n",
    "        g.ndata[\"h\"] = feats\n",
    "        g.update_all(fn.copy_u(\"h\", \"m\"), fn.sum(\"m\", \"h\"))\n",
    "        feats = g.ndata.pop(\"h\")\n",
    "        feats = feats * norm\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_and_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import dgl\n",
    "\n",
    "\"\"\"\n",
    "1. Train and eval\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train(model, data, feats, labels, criterion, optimizer, idx_train, lamb=1):\n",
    "    \"\"\"\n",
    "    GNN full-batch training. Input the entire graph `g` as data.\n",
    "    lamb: weight parameter lambda\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # Compute loss and prediction\n",
    "    logits = model(data, feats)\n",
    "    out = logits.log_softmax(dim=1)\n",
    "    loss = criterion(out[idx_train], labels[idx_train])\n",
    "    loss_val = loss.item()\n",
    "\n",
    "    loss *= lamb\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss_val\n",
    "\n",
    "\n",
    "def train_sage(model, dataloader, feats, labels, criterion, optimizer, lamb=1):\n",
    "    \"\"\"\n",
    "    Train for GraphSAGE. Process the graph in mini-batches using `dataloader` instead the entire graph `g`.\n",
    "    lamb: weight parameter lambda\n",
    "    \"\"\"\n",
    "    device = feats.device\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n",
    "        blocks = [blk.int().to(device) for blk in blocks]\n",
    "        batch_feats = feats[input_nodes]\n",
    "        batch_labels = labels[output_nodes]\n",
    "\n",
    "        # Compute loss and prediction\n",
    "        logits = model(blocks, batch_feats)\n",
    "        out = logits.log_softmax(dim=1)\n",
    "        loss = criterion(out, batch_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss *= lamb\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def train_mini_batch(model, feats, labels, batch_size, criterion, optimizer, lamb=1):\n",
    "    \"\"\"\n",
    "    Train MLP for large datasets. Process the data in mini-batches. The graph is ignored, node features only.\n",
    "    lamb: weight parameter lambda\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    num_batches = max(1, feats.shape[0] // batch_size)\n",
    "    idx_batch = torch.randperm(feats.shape[0])[: num_batches * batch_size]\n",
    "\n",
    "    if num_batches == 1:\n",
    "        idx_batch = idx_batch.view(1, -1)\n",
    "    else:\n",
    "        idx_batch = idx_batch.view(num_batches, batch_size)\n",
    "\n",
    "    total_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        # No graph needed for the forward function\n",
    "        logits = model(None, feats[idx_batch[i]])\n",
    "        out = logits.log_softmax(dim=1)\n",
    "\n",
    "        loss = criterion(out, labels[idx_batch[i]])\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss *= lamb\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate(model, data, feats, labels, criterion, evaluator, idx_eval=None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    out: log probability of all input data\n",
    "    loss & score (float): evaluated loss & score, if idx_eval is not None, only loss & score on those idx.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model.inference(data, feats)\n",
    "        out = logits.log_softmax(dim=1)\n",
    "        if idx_eval is None:\n",
    "            loss = criterion(out, labels)\n",
    "            score = evaluator(out, labels)\n",
    "        else:\n",
    "            loss = criterion(out[idx_eval], labels[idx_eval])\n",
    "            score = evaluator(out[idx_eval], labels[idx_eval])\n",
    "    return out, loss.item(), score\n",
    "\n",
    "\n",
    "def evaluate_mini_batch(\n",
    "    model, feats, labels, criterion, batch_size, evaluator, idx_eval=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate MLP for large datasets. Process the data in mini-batches. The graph is ignored, node features only.\n",
    "    Return:\n",
    "    out: log probability of all input data\n",
    "    loss & score (float): evaluated loss & score, if idx_eval is not None, only loss & score on those idx.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        num_batches = int(np.ceil(len(feats) / batch_size))\n",
    "        out_list = []\n",
    "        for i in range(num_batches):\n",
    "            logits = model.inference(None, feats[batch_size * i : batch_size * (i + 1)])\n",
    "            out = logits.log_softmax(dim=1)\n",
    "            out_list += [out.detach()]\n",
    "\n",
    "        out_all = torch.cat(out_list)\n",
    "\n",
    "        if idx_eval is None:\n",
    "            loss = criterion(out_all, labels)\n",
    "            score = evaluator(out_all, labels)\n",
    "        else:\n",
    "            loss = criterion(out_all[idx_eval], labels[idx_eval])\n",
    "            score = evaluator(out_all[idx_eval], labels[idx_eval])\n",
    "\n",
    "    return out_all, loss.item(), score\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2. Run teacher\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_transductive(\n",
    "    conf,\n",
    "    model,\n",
    "    g,\n",
    "    feats,\n",
    "    labels,\n",
    "    indices,\n",
    "    criterion,\n",
    "    evaluator,\n",
    "    optimizer,\n",
    "    logger,\n",
    "    loss_and_score,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and eval under the transductive setting.\n",
    "    The train/valid/test split is specified by `indices`.\n",
    "    The input graph is assumed to be large. Thus, SAGE is used for GNNs, mini-batch is used for MLPs.\n",
    "\n",
    "    loss_and_score: Stores losses and scores.\n",
    "    \"\"\"\n",
    "    set_seed(conf[\"seed\"])\n",
    "    device = conf[\"device\"]\n",
    "    batch_size = conf[\"batch_size\"]\n",
    "\n",
    "    idx_train, idx_val, idx_test = indices\n",
    "\n",
    "    feats = feats.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    if \"SAGE\" in model.model_name:\n",
    "        # Create dataloader for SAGE\n",
    "\n",
    "        # Create csr/coo/csc formats before launching sampling processes\n",
    "        # This avoids creating certain formats in each data loader process, which saves momory and CPU.\n",
    "        g.create_formats_()\n",
    "        sampler = dgl.dataloading.MultiLayerNeighborSampler(\n",
    "            [eval(fanout) for fanout in conf[\"fan_out\"].split(\",\")]\n",
    "        )\n",
    "        dataloader = dgl.dataloading.NodeDataLoader(\n",
    "            g,\n",
    "            idx_train,\n",
    "            sampler,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=conf[\"num_workers\"],\n",
    "        )\n",
    "\n",
    "        # SAGE inference is implemented as layer by layer, so the full-neighbor sampler only collects one-hop neighors\n",
    "        sampler_eval = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n",
    "        dataloader_eval = dgl.dataloading.NodeDataLoader(\n",
    "            g,\n",
    "            torch.arange(g.num_nodes()),\n",
    "            sampler_eval,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=conf[\"num_workers\"],\n",
    "        )\n",
    "\n",
    "        data = dataloader\n",
    "        data_eval = dataloader_eval\n",
    "    elif \"MLP\" in model.model_name:\n",
    "        feats_train, labels_train = feats[idx_train], labels[idx_train]\n",
    "        feats_val, labels_val = feats[idx_val], labels[idx_val]\n",
    "        feats_test, labels_test = feats[idx_test], labels[idx_test]\n",
    "    else:\n",
    "        g = g.to(device)\n",
    "        data = g\n",
    "        data_eval = g\n",
    "\n",
    "    best_epoch, best_score_val, count = 0, 0, 0\n",
    "    for epoch in range(1, conf[\"max_epoch\"] + 1):\n",
    "        if \"SAGE\" in model.model_name:\n",
    "            loss = train_sage(model, data, feats, labels, criterion, optimizer)\n",
    "        elif \"MLP\" in model.model_name:\n",
    "            loss = train_mini_batch(\n",
    "                model, feats_train, labels_train, batch_size, criterion, optimizer\n",
    "            )\n",
    "        else:\n",
    "            loss = train(model, data, feats, labels, criterion, optimizer, idx_train)\n",
    "\n",
    "        if epoch % conf[\"eval_interval\"] == 0:\n",
    "            if \"MLP\" in model.model_name:\n",
    "                _, loss_train, score_train = evaluate_mini_batch(\n",
    "                    model, feats_train, labels_train, criterion, batch_size, evaluator\n",
    "                )\n",
    "                _, loss_val, score_val = evaluate_mini_batch(\n",
    "                    model, feats_val, labels_val, criterion, batch_size, evaluator\n",
    "                )\n",
    "                _, loss_test, score_test = evaluate_mini_batch(\n",
    "                    model, feats_test, labels_test, criterion, batch_size, evaluator\n",
    "                )\n",
    "            else:\n",
    "                out, loss_train, score_train = evaluate(\n",
    "                    model, data_eval, feats, labels, criterion, evaluator, idx_train\n",
    "                )\n",
    "                # Use criterion & evaluator instead of evaluate to avoid redundant forward pass\n",
    "                loss_val = criterion(out[idx_val], labels[idx_val]).item()\n",
    "                score_val = evaluator(out[idx_val], labels[idx_val])\n",
    "                loss_test = criterion(out[idx_test], labels[idx_test]).item()\n",
    "                score_test = evaluator(out[idx_test], labels[idx_test])\n",
    "\n",
    "            logger.debug(\n",
    "                f\"Ep {epoch:3d} | loss: {loss:.4f} | s_train: {score_train:.4f} | s_val: {score_val:.4f} | s_test: {score_test:.4f}\"\n",
    "            )\n",
    "            loss_and_score += [\n",
    "                [\n",
    "                    epoch,\n",
    "                    loss_train,\n",
    "                    loss_val,\n",
    "                    loss_test,\n",
    "                    score_train,\n",
    "                    score_val,\n",
    "                    score_test,\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "            if score_val >= best_score_val:\n",
    "                best_epoch = epoch\n",
    "                best_score_val = score_val\n",
    "                state = copy.deepcopy(model.state_dict())\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "\n",
    "        if count == conf[\"patience\"] or epoch == conf[\"max_epoch\"]:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(state)\n",
    "    if \"MLP\" in model.model_name:\n",
    "        out, _, score_val = evaluate_mini_batch(\n",
    "            model, feats, labels, criterion, batch_size, evaluator, idx_val\n",
    "        )\n",
    "    else:\n",
    "        out, _, score_val = evaluate(\n",
    "            model, data_eval, feats, labels, criterion, evaluator, idx_val\n",
    "        )\n",
    "\n",
    "    score_test = evaluator(out[idx_test], labels[idx_test])\n",
    "    logger.info(\n",
    "        f\"Best valid model at epoch: {best_epoch: 3d}, score_val: {score_val :.4f}, score_test: {score_test :.4f}\"\n",
    "    )\n",
    "    return out, score_val, score_test\n",
    "\n",
    "\n",
    "def run_inductive(\n",
    "    conf,\n",
    "    model,\n",
    "    g,\n",
    "    feats,\n",
    "    labels,\n",
    "    indices,\n",
    "    criterion,\n",
    "    evaluator,\n",
    "    optimizer,\n",
    "    logger,\n",
    "    loss_and_score,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and eval under the inductive setting.\n",
    "    The train/valid/test split is specified by `indices`.\n",
    "    idx starting with `obs_idx_` contains the node idx in the observed graph `obs_g`.\n",
    "    idx starting with `idx_` contains the node idx in the original graph `g`.\n",
    "    The model is trained on the observed graph `obs_g`, and evaluated on both the observed test nodes (`obs_idx_test`) and inductive test nodes (`idx_test_ind`).\n",
    "    The input graph is assumed to be large. Thus, SAGE is used for GNNs, mini-batch is used for MLPs.\n",
    "\n",
    "    idx_obs: Idx of nodes in the original graph `g`, which form the observed graph 'obs_g'.\n",
    "    loss_and_score: Stores losses and scores.\n",
    "    \"\"\"\n",
    "\n",
    "    set_seed(conf[\"seed\"])\n",
    "    device = conf[\"device\"]\n",
    "    batch_size = conf[\"batch_size\"]\n",
    "    obs_idx_train, obs_idx_val, obs_idx_test, idx_obs, idx_test_ind = indices\n",
    "\n",
    "    feats = feats.to(device)\n",
    "    labels = labels.to(device)\n",
    "    obs_feats = feats[idx_obs]\n",
    "    obs_labels = labels[idx_obs]\n",
    "    obs_g = g.subgraph(idx_obs)\n",
    "\n",
    "    if \"SAGE\" in model.model_name:\n",
    "        # Create dataloader for SAGE\n",
    "\n",
    "        # Create csr/coo/csc formats before launching sampling processes\n",
    "        # This avoids creating certain formats in each data loader process, which saves momory and CPU.\n",
    "        obs_g.create_formats_()\n",
    "        g.create_formats_()\n",
    "        sampler = dgl.dataloading.MultiLayerNeighborSampler(\n",
    "            [eval(fanout) for fanout in conf[\"fan_out\"].split(\",\")]\n",
    "        )\n",
    "        obs_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "            obs_g,\n",
    "            obs_idx_train,\n",
    "            sampler,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=conf[\"num_workers\"],\n",
    "        )\n",
    "\n",
    "        sampler_eval = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n",
    "        obs_dataloader_eval = dgl.dataloading.NodeDataLoader(\n",
    "            obs_g,\n",
    "            torch.arange(obs_g.num_nodes()),\n",
    "            sampler_eval,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=conf[\"num_workers\"],\n",
    "        )\n",
    "        dataloader_eval = dgl.dataloading.NodeDataLoader(\n",
    "            g,\n",
    "            torch.arange(g.num_nodes()),\n",
    "            sampler_eval,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=conf[\"num_workers\"],\n",
    "        )\n",
    "\n",
    "        obs_data = obs_dataloader\n",
    "        obs_data_eval = obs_dataloader_eval\n",
    "        data_eval = dataloader_eval\n",
    "    elif \"MLP\" in model.model_name:\n",
    "        feats_train, labels_train = obs_feats[obs_idx_train], obs_labels[obs_idx_train]\n",
    "        feats_val, labels_val = obs_feats[obs_idx_val], obs_labels[obs_idx_val]\n",
    "        feats_test_tran, labels_test_tran = (\n",
    "            obs_feats[obs_idx_test],\n",
    "            obs_labels[obs_idx_test],\n",
    "        )\n",
    "        feats_test_ind, labels_test_ind = feats[idx_test_ind], labels[idx_test_ind]\n",
    "\n",
    "    else:\n",
    "        obs_g = obs_g.to(device)\n",
    "        g = g.to(device)\n",
    "\n",
    "        obs_data = obs_g\n",
    "        obs_data_eval = obs_g\n",
    "        data_eval = g\n",
    "\n",
    "    best_epoch, best_score_val, count = 0, 0, 0\n",
    "    for epoch in range(1, conf[\"max_epoch\"] + 1):\n",
    "        if \"SAGE\" in model.model_name:\n",
    "            loss = train_sage(\n",
    "                model, obs_data, obs_feats, obs_labels, criterion, optimizer\n",
    "            )\n",
    "        elif \"MLP\" in model.model_name:\n",
    "            loss = train_mini_batch(\n",
    "                model, feats_train, labels_train, batch_size, criterion, optimizer\n",
    "            )\n",
    "        else:\n",
    "            loss = train(\n",
    "                model,\n",
    "                obs_data,\n",
    "                obs_feats,\n",
    "                obs_labels,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                obs_idx_train,\n",
    "            )\n",
    "\n",
    "        if epoch % conf[\"eval_interval\"] == 0:\n",
    "            if \"MLP\" in model.model_name:\n",
    "                _, loss_train, score_train = evaluate_mini_batch(\n",
    "                    model, feats_train, labels_train, criterion, batch_size, evaluator\n",
    "                )\n",
    "                _, loss_val, score_val = evaluate_mini_batch(\n",
    "                    model, feats_val, labels_val, criterion, batch_size, evaluator\n",
    "                )\n",
    "                _, loss_test_tran, score_test_tran = evaluate_mini_batch(\n",
    "                    model,\n",
    "                    feats_test_tran,\n",
    "                    labels_test_tran,\n",
    "                    criterion,\n",
    "                    batch_size,\n",
    "                    evaluator,\n",
    "                )\n",
    "                _, loss_test_ind, score_test_ind = evaluate_mini_batch(\n",
    "                    model,\n",
    "                    feats_test_ind,\n",
    "                    labels_test_ind,\n",
    "                    criterion,\n",
    "                    batch_size,\n",
    "                    evaluator,\n",
    "                )\n",
    "            else:\n",
    "                obs_out, loss_train, score_train = evaluate(\n",
    "                    model,\n",
    "                    obs_data_eval,\n",
    "                    obs_feats,\n",
    "                    obs_labels,\n",
    "                    criterion,\n",
    "                    evaluator,\n",
    "                    obs_idx_train,\n",
    "                )\n",
    "                # Use criterion & evaluator instead of evaluate to avoid redundant forward pass\n",
    "                loss_val = criterion(\n",
    "                    obs_out[obs_idx_val], obs_labels[obs_idx_val]\n",
    "                ).item()\n",
    "                score_val = evaluator(obs_out[obs_idx_val], obs_labels[obs_idx_val])\n",
    "                loss_test_tran = criterion(\n",
    "                    obs_out[obs_idx_test], obs_labels[obs_idx_test]\n",
    "                ).item()\n",
    "                score_test_tran = evaluator(\n",
    "                    obs_out[obs_idx_test], obs_labels[obs_idx_test]\n",
    "                )\n",
    "\n",
    "                # Evaluate the inductive part with the full graph\n",
    "                out, loss_test_ind, score_test_ind = evaluate(\n",
    "                    model, data_eval, feats, labels, criterion, evaluator, idx_test_ind\n",
    "                )\n",
    "            logger.debug(\n",
    "                f\"Ep {epoch:3d} | loss: {loss:.4f} | s_train: {score_train:.4f} | s_val: {score_val:.4f} | s_tt: {score_test_tran:.4f} | s_ti: {score_test_ind:.4f}\"\n",
    "            )\n",
    "            loss_and_score += [\n",
    "                [\n",
    "                    epoch,\n",
    "                    loss_train,\n",
    "                    loss_val,\n",
    "                    loss_test_tran,\n",
    "                    loss_test_ind,\n",
    "                    score_train,\n",
    "                    score_val,\n",
    "                    score_test_tran,\n",
    "                    score_test_ind,\n",
    "                ]\n",
    "            ]\n",
    "            if score_val >= best_score_val:\n",
    "                best_epoch = epoch\n",
    "                best_score_val = score_val\n",
    "                state = copy.deepcopy(model.state_dict())\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "\n",
    "        if count == conf[\"patience\"] or epoch == conf[\"max_epoch\"]:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(state)\n",
    "    if \"MLP\" in model.model_name:\n",
    "        obs_out, _, score_val = evaluate_mini_batch(\n",
    "            model, obs_feats, obs_labels, criterion, batch_size, evaluator, obs_idx_val\n",
    "        )\n",
    "        out, _, score_test_ind = evaluate_mini_batch(\n",
    "            model, feats, labels, criterion, batch_size, evaluator, idx_test_ind\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        obs_out, _, score_val = evaluate(\n",
    "            model,\n",
    "            obs_data_eval,\n",
    "            obs_feats,\n",
    "            obs_labels,\n",
    "            criterion,\n",
    "            evaluator,\n",
    "            obs_idx_val,\n",
    "        )\n",
    "        out, _, score_test_ind = evaluate(\n",
    "            model, data_eval, feats, labels, criterion, evaluator, idx_test_ind\n",
    "        )\n",
    "\n",
    "    score_test_tran = evaluator(obs_out[obs_idx_test], obs_labels[obs_idx_test])\n",
    "    out[idx_obs] = obs_out\n",
    "    logger.info(\n",
    "        f\"Best valid model at epoch: {best_epoch :3d}, score_val: {score_val :.4f}, score_test_tran: {score_test_tran :.4f}, score_test_ind: {score_test_ind :.4f}\"\n",
    "    )\n",
    "    return out, score_val, score_test_tran, score_test_ind\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3. Distill\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def distill_run_transductive(\n",
    "    conf,\n",
    "    model,\n",
    "    feats,\n",
    "    labels,\n",
    "    out_t_all,\n",
    "    distill_indices,\n",
    "    criterion_l,\n",
    "    criterion_t,\n",
    "    evaluator,\n",
    "    optimizer,\n",
    "    logger,\n",
    "    loss_and_score,\n",
    "):\n",
    "    \"\"\"\n",
    "    Distill training and eval under the transductive setting.\n",
    "    The hard_label_train/soft_label_train/valid/test split is specified by `distill_indices`.\n",
    "    The input graph is assumed to be large, and MLP is assumed to be the student model. Thus, node feature only and mini-batch is used.\n",
    "\n",
    "    out_t: Soft labels produced by the teacher model.\n",
    "    criterion_l & criterion_t: Loss used for hard labels (`labels`) and soft labels (`out_t`) respectively\n",
    "    loss_and_score: Stores losses and scores.\n",
    "    \"\"\"\n",
    "    set_seed(conf[\"seed\"])\n",
    "    device = conf[\"device\"]\n",
    "    batch_size = conf[\"batch_size\"]\n",
    "    lamb = conf[\"lamb\"]\n",
    "    idx_l, idx_t, idx_val, idx_test = distill_indices\n",
    "\n",
    "    feats = feats.to(device)\n",
    "    labels = labels.to(device)\n",
    "    out_t_all = out_t_all.to(device)\n",
    "\n",
    "    feats_l, labels_l = feats[idx_l], labels[idx_l]\n",
    "    feats_t, out_t = feats[idx_t], out_t_all[idx_t]\n",
    "    feats_val, labels_val = feats[idx_val], labels[idx_val]\n",
    "    feats_test, labels_test = feats[idx_test], labels[idx_test]\n",
    "\n",
    "    best_epoch, best_score_val, count = 0, 0, 0\n",
    "    for epoch in range(1, conf[\"max_epoch\"] + 1):\n",
    "        loss_l = train_mini_batch(\n",
    "            model, feats_l, labels_l, batch_size, criterion_l, optimizer, lamb\n",
    "        )\n",
    "        loss_t = train_mini_batch(\n",
    "            model, feats_t, out_t, batch_size, criterion_t, optimizer, 1 - lamb\n",
    "        )\n",
    "        loss = loss_l + loss_t\n",
    "        if epoch % conf[\"eval_interval\"] == 0:\n",
    "            _, loss_l, score_l = evaluate_mini_batch(\n",
    "                model, feats_l, labels_l, criterion_l, batch_size, evaluator\n",
    "            )\n",
    "            _, loss_val, score_val = evaluate_mini_batch(\n",
    "                model, feats_val, labels_val, criterion_l, batch_size, evaluator\n",
    "            )\n",
    "            _, loss_test, score_test = evaluate_mini_batch(\n",
    "                model, feats_test, labels_test, criterion_l, batch_size, evaluator\n",
    "            )\n",
    "\n",
    "            logger.debug(\n",
    "                f\"Ep {epoch:3d} | loss: {loss:.4f} | s_l: {score_l:.4f} | s_val: {score_val:.4f} | s_test: {score_test:.4f}\"\n",
    "            )\n",
    "            loss_and_score += [\n",
    "                [epoch, loss_l, loss_val, loss_test, score_l, score_val, score_test]\n",
    "            ]\n",
    "\n",
    "            if score_val >= best_score_val:\n",
    "                best_epoch = epoch\n",
    "                best_score_val = score_val\n",
    "                state = copy.deepcopy(model.state_dict())\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "\n",
    "        if count == conf[\"patience\"] or epoch == conf[\"max_epoch\"]:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(state)\n",
    "    out, _, score_val = evaluate_mini_batch(\n",
    "        model, feats, labels, criterion_l, batch_size, evaluator, idx_val\n",
    "    )\n",
    "    # Use evaluator instead of evaluate to avoid redundant forward pass\n",
    "    score_test = evaluator(out[idx_test], labels_test)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Best valid model at epoch: {best_epoch: 3d}, score_val: {score_val :.4f}, score_test: {score_test :.4f}\"\n",
    "    )\n",
    "    return out, score_val, score_test\n",
    "\n",
    "\n",
    "def distill_run_inductive(\n",
    "    conf,\n",
    "    model,\n",
    "    feats,\n",
    "    labels,\n",
    "    out_t_all,\n",
    "    distill_indices,\n",
    "    criterion_l,\n",
    "    criterion_t,\n",
    "    evaluator,\n",
    "    optimizer,\n",
    "    logger,\n",
    "    loss_and_score,\n",
    "):\n",
    "    \"\"\"\n",
    "    Distill training and eval under the inductive setting.\n",
    "    The hard_label_train/soft_label_train/valid/test split is specified by `distill_indices`.\n",
    "    idx starting with `obs_idx_` contains the node idx in the observed graph `obs_g`.\n",
    "    idx starting with `idx_` contains the node idx in the original graph `g`.\n",
    "    The model is trained on the observed graph `obs_g`, and evaluated on both the observed test nodes (`obs_idx_test`) and inductive test nodes (`idx_test_ind`).\n",
    "    The input graph is assumed to be large, and MLP is assumed to be the student model. Thus, node feature only and mini-batch is used.\n",
    "\n",
    "    idx_obs: Idx of nodes in the original graph `g`, which form the observed graph 'obs_g'.\n",
    "    out_t: Soft labels produced by the teacher model.\n",
    "    criterion_l & criterion_t: Loss used for hard labels (`labels`) and soft labels (`out_t`) respectively.\n",
    "    loss_and_score: Stores losses and scores.\n",
    "    \"\"\"\n",
    "\n",
    "    set_seed(conf[\"seed\"])\n",
    "    device = conf[\"device\"]\n",
    "    batch_size = conf[\"batch_size\"]\n",
    "    lamb = conf[\"lamb\"]\n",
    "    (\n",
    "        obs_idx_l,\n",
    "        obs_idx_t,\n",
    "        obs_idx_val,\n",
    "        obs_idx_test,\n",
    "        idx_obs,\n",
    "        idx_test_ind,\n",
    "    ) = distill_indices\n",
    "\n",
    "    feats = feats.to(device)\n",
    "    labels = labels.to(device)\n",
    "    out_t_all = out_t_all.to(device)\n",
    "    obs_feats = feats[idx_obs]\n",
    "    obs_labels = labels[idx_obs]\n",
    "    obs_out_t = out_t_all[idx_obs]\n",
    "\n",
    "    feats_l, labels_l = obs_feats[obs_idx_l], obs_labels[obs_idx_l]\n",
    "    feats_t, out_t = obs_feats[obs_idx_t], obs_out_t[obs_idx_t]\n",
    "    feats_val, labels_val = obs_feats[obs_idx_val], obs_labels[obs_idx_val]\n",
    "    feats_test_tran, labels_test_tran = (\n",
    "        obs_feats[obs_idx_test],\n",
    "        obs_labels[obs_idx_test],\n",
    "    )\n",
    "    feats_test_ind, labels_test_ind = feats[idx_test_ind], labels[idx_test_ind]\n",
    "\n",
    "    best_epoch, best_score_val, count = 0, 0, 0\n",
    "    for epoch in range(1, conf[\"max_epoch\"] + 1):\n",
    "        loss_l = train_mini_batch(\n",
    "            model, feats_l, labels_l, batch_size, criterion_l, optimizer, lamb\n",
    "        )\n",
    "        loss_t = train_mini_batch(\n",
    "            model, feats_t, out_t, batch_size, criterion_t, optimizer, 1 - lamb\n",
    "        )\n",
    "        loss = loss_l + loss_t\n",
    "        if epoch % conf[\"eval_interval\"] == 0:\n",
    "            _, loss_l, score_l = evaluate_mini_batch(\n",
    "                model, feats_l, labels_l, criterion_l, batch_size, evaluator\n",
    "            )\n",
    "            _, loss_val, score_val = evaluate_mini_batch(\n",
    "                model, feats_val, labels_val, criterion_l, batch_size, evaluator\n",
    "            )\n",
    "            _, loss_test_tran, score_test_tran = evaluate_mini_batch(\n",
    "                model,\n",
    "                feats_test_tran,\n",
    "                labels_test_tran,\n",
    "                criterion_l,\n",
    "                batch_size,\n",
    "                evaluator,\n",
    "            )\n",
    "            _, loss_test_ind, score_test_ind = evaluate_mini_batch(\n",
    "                model,\n",
    "                feats_test_ind,\n",
    "                labels_test_ind,\n",
    "                criterion_l,\n",
    "                batch_size,\n",
    "                evaluator,\n",
    "            )\n",
    "\n",
    "            logger.debug(\n",
    "                f\"Ep {epoch:3d} | l: {loss:.4f} | s_l: {score_l:.4f} | s_val: {score_val:.4f} | s_tt: {score_test_tran:.4f} | s_ti: {score_test_ind:.4f}\"\n",
    "            )\n",
    "            loss_and_score += [\n",
    "                [\n",
    "                    epoch,\n",
    "                    loss_l,\n",
    "                    loss_val,\n",
    "                    loss_test_tran,\n",
    "                    loss_test_ind,\n",
    "                    score_l,\n",
    "                    score_val,\n",
    "                    score_test_tran,\n",
    "                    score_test_ind,\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "            if score_val >= best_score_val:\n",
    "                best_epoch = epoch\n",
    "                best_score_val = score_val\n",
    "                state = copy.deepcopy(model.state_dict())\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "\n",
    "        if count == conf[\"patience\"] or epoch == conf[\"max_epoch\"]:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(state)\n",
    "    obs_out, _, score_val = evaluate_mini_batch(\n",
    "        model, obs_feats, obs_labels, criterion_l, batch_size, evaluator, obs_idx_val\n",
    "    )\n",
    "    out, _, score_test_ind = evaluate_mini_batch(\n",
    "        model, feats, labels, criterion_l, batch_size, evaluator, idx_test_ind\n",
    "    )\n",
    "\n",
    "    # Use evaluator instead of evaluate to avoid redundant forward pass\n",
    "    score_test_tran = evaluator(obs_out[obs_idx_test], labels_test_tran)\n",
    "    out[idx_obs] = obs_out\n",
    "\n",
    "    logger.info(\n",
    "        f\"Best valid model at epoch: {best_epoch: 3d} score_val: {score_val :.4f}, score_test_tran: {score_test_tran :.4f}, score_test_ind: {score_test_ind :.4f}\"\n",
    "    )\n",
    "    return out, score_val, score_test_tran, score_test_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from the CPF implementation\n",
    "https://github.com/BUPT-GAMMA/CPF/tree/389c01aaf238689ee7b1e5aba127842341e123b6/data\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "\n",
    "\n",
    "def is_binary_bag_of_words(features):\n",
    "    features_coo = features.tocoo()\n",
    "    return all(\n",
    "        single_entry == 1.0\n",
    "        for _, _, single_entry in zip(\n",
    "            features_coo.row, features_coo.col, features_coo.data\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def to_binary_bag_of_words(features):\n",
    "    \"\"\"Converts TF/IDF features to binary bag-of-words features.\"\"\"\n",
    "    features_copy = features.tocsr()\n",
    "    features_copy.data[:] = 1.0\n",
    "    return features_copy\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.0\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    return adj\n",
    "\n",
    "\n",
    "def eliminate_self_loops_adj(A):\n",
    "    \"\"\"Remove self-loops from the adjacency matrix.\"\"\"\n",
    "    A = A.tolil()\n",
    "    A.setdiag(0)\n",
    "    A = A.tocsr()\n",
    "    A.eliminate_zeros()\n",
    "    return A\n",
    "\n",
    "\n",
    "def largest_connected_components(sparse_graph, n_components=1):\n",
    "    \"\"\"Select the largest connected components in the graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sparse_graph : SparseGraph\n",
    "        Input graph.\n",
    "    n_components : int, default 1\n",
    "        Number of largest connected components to keep.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sparse_graph : SparseGraph\n",
    "        Subgraph of the input graph where only the nodes in largest n_components are kept.\n",
    "\n",
    "    \"\"\"\n",
    "    _, component_indices = sp.csgraph.connected_components(sparse_graph.adj_matrix)\n",
    "    component_sizes = np.bincount(component_indices)\n",
    "    components_to_keep = np.argsort(component_sizes)[::-1][\n",
    "        :n_components\n",
    "    ]  # reverse order to sort descending\n",
    "    nodes_to_keep = [\n",
    "        idx\n",
    "        for (idx, component) in enumerate(component_indices)\n",
    "        if component in components_to_keep\n",
    "    ]\n",
    "    return create_subgraph(sparse_graph, nodes_to_keep=nodes_to_keep)\n",
    "\n",
    "\n",
    "def create_subgraph(\n",
    "    sparse_graph, _sentinel=None, nodes_to_remove=None, nodes_to_keep=None\n",
    "):\n",
    "    \"\"\"Create a graph with the specified subset of nodes.\n",
    "\n",
    "    Exactly one of (nodes_to_remove, nodes_to_keep) should be provided, while the other stays None.\n",
    "    Note that to avoid confusion, it is required to pass node indices as named arguments to this function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sparse_graph : SparseGraph\n",
    "        Input graph.\n",
    "    _sentinel : None\n",
    "        Internal, to prevent passing positional arguments. Do not use.\n",
    "    nodes_to_remove : array-like of int\n",
    "        Indices of nodes that have to removed.\n",
    "    nodes_to_keep : array-like of int\n",
    "        Indices of nodes that have to be kept.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sparse_graph : SparseGraph\n",
    "        Graph with specified nodes removed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check that arguments are passed correctly\n",
    "    if _sentinel is not None:\n",
    "        raise ValueError(\n",
    "            \"Only call `create_subgraph` with named arguments',\"\n",
    "            \" (nodes_to_remove=...) or (nodes_to_keep=...)\"\n",
    "        )\n",
    "    if nodes_to_remove is None and nodes_to_keep is None:\n",
    "        raise ValueError(\"Either nodes_to_remove or nodes_to_keep must be provided.\")\n",
    "    elif nodes_to_remove is not None and nodes_to_keep is not None:\n",
    "        raise ValueError(\n",
    "            \"Only one of nodes_to_remove or nodes_to_keep must be provided.\"\n",
    "        )\n",
    "    elif nodes_to_remove is not None:\n",
    "        nodes_to_keep = [\n",
    "            i for i in range(sparse_graph.num_nodes()) if i not in nodes_to_remove\n",
    "        ]\n",
    "    elif nodes_to_keep is not None:\n",
    "        nodes_to_keep = sorted(nodes_to_keep)\n",
    "    else:\n",
    "        raise RuntimeError(\"This should never happen.\")\n",
    "\n",
    "    sparse_graph.adj_matrix = sparse_graph.adj_matrix[nodes_to_keep][:, nodes_to_keep]\n",
    "    if sparse_graph.attr_matrix is not None:\n",
    "        sparse_graph.attr_matrix = sparse_graph.attr_matrix[nodes_to_keep]\n",
    "    if sparse_graph.labels is not None:\n",
    "        sparse_graph.labels = sparse_graph.labels[nodes_to_keep]\n",
    "    if sparse_graph.node_names is not None:\n",
    "        sparse_graph.node_names = sparse_graph.node_names[nodes_to_keep]\n",
    "    return sparse_graph\n",
    "\n",
    "\n",
    "def binarize_labels(labels, sparse_output=False, return_classes=False):\n",
    "    \"\"\"Convert labels vector to a binary label matrix.\n",
    "\n",
    "    In the default single-label case, labels look like\n",
    "    labels = [y1, y2, y3, ...].\n",
    "    Also supports the multi-label format.\n",
    "    In this case, labels should look something like\n",
    "    labels = [[y11, y12], [y21, y22, y23], [y31], ...].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : array-like, shape [num_samples]\n",
    "        Array of node labels in categorical single- or multi-label format.\n",
    "    sparse_output : bool, default False\n",
    "        Whether return the label_matrix in CSR format.\n",
    "    return_classes : bool, default False\n",
    "        Whether return the classes corresponding to the columns of the label matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    label_matrix : np.ndarray or sp.csr_matrix, shape [num_samples, num_classes]\n",
    "        Binary matrix of class labels.\n",
    "        num_classes = number of unique values in \"labels\" array.\n",
    "        label_matrix[i, k] = 1 <=> node i belongs to class k.\n",
    "    classes : np.array, shape [num_classes], optional\n",
    "        Classes that correspond to each column of the label_matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    if hasattr(labels[0], \"__iter__\"):  # labels[0] is iterable <=> multilabel format\n",
    "        binarizer = MultiLabelBinarizer(sparse_output=sparse_output)\n",
    "    else:\n",
    "        binarizer = LabelBinarizer(sparse_output=sparse_output)\n",
    "    label_matrix = binarizer.fit_transform(labels).astype(np.float32)\n",
    "    return (label_matrix, binarizer.classes_) if return_classes else label_matrix\n",
    "\n",
    "\n",
    "def remove_underrepresented_classes(\n",
    "    g, train_examples_per_class, val_examples_per_class\n",
    "):\n",
    "    \"\"\"Remove nodes from graph that correspond to a class of which there are less than\n",
    "    num_classes * train_examples_per_class + num_classes * val_examples_per_class nodes.\n",
    "\n",
    "    Those classes would otherwise break the training procedure.\n",
    "    \"\"\"\n",
    "    min_examples_per_class = train_examples_per_class + val_examples_per_class\n",
    "    examples_counter = Counter(g.labels)\n",
    "    keep_classes = set(\n",
    "        class_\n",
    "        for class_, count in examples_counter.items()\n",
    "        if count > min_examples_per_class\n",
    "    )\n",
    "    keep_indices = [i for i in range(len(g.labels)) if g.labels[i] in keep_classes]\n",
    "\n",
    "    return create_subgraph(g, nodes_to_keep=keep_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataloader of CPF datasets are adapted from the CPF implementation\n",
    "https://github.com/BUPT-GAMMA/CPF/tree/389c01aaf238689ee7b1e5aba127842341e123b6/data\n",
    "\n",
    "Dataloader of NonHom datasets are adapted from the Non-homophily benchmarks\n",
    "https://github.com/CUAI/Non-Homophily-Benchmarks\n",
    "\n",
    "Dataloader of BGNN datasets are adapted from the BGNN implementation and dgl example of BGNN\n",
    "https://github.com/nd7141/bgnn\n",
    "https://github.com/dmlc/dgl/tree/473d5e0a4c4e4735f1c9dc9d783e0374328cca9a/examples/pytorch/bgnn\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import dgl\n",
    "import os\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import json\n",
    "from dgl.data.utils import load_graphs\n",
    "from os import path\n",
    "from category_encoders import CatBoostEncoder\n",
    "from pathlib import Path\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import preprocessing\n",
    "from data_preprocess import (\n",
    "    normalize_adj,\n",
    "    eliminate_self_loops_adj,\n",
    "    largest_connected_components,\n",
    "    binarize_labels,\n",
    ")\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "CPF_data = [\"cora\", \"citeseer\", \"pubmed\", \"a-computer\", \"a-photo\"]\n",
    "OGB_data = [\"ogbn-arxiv\", \"ogbn-products\"]\n",
    "NonHom_data = [\"pokec\", \"penn94\"]\n",
    "BGNN_data = [\"house_class\", \"vk_class\"]\n",
    "\n",
    "\n",
    "def load_data(dataset, dataset_path, **kwargs):\n",
    "    if dataset in CPF_data:\n",
    "        return load_cpf_data(\n",
    "            dataset,\n",
    "            dataset_path,\n",
    "            kwargs[\"seed\"],\n",
    "            kwargs[\"labelrate_train\"],\n",
    "            kwargs[\"labelrate_val\"],\n",
    "        )\n",
    "    elif dataset in OGB_data:\n",
    "        return load_ogb_data(dataset, dataset_path)\n",
    "    elif dataset in NonHom_data:\n",
    "        return load_nonhom_data(dataset, dataset_path, kwargs[\"split_idx\"])\n",
    "    elif dataset in BGNN_data:\n",
    "        return load_bgnn_data(dataset, dataset_path, kwargs[\"split_idx\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset}\")\n",
    "\n",
    "\n",
    "def load_ogb_data(dataset, dataset_path):\n",
    "    data = DglNodePropPredDataset(dataset, dataset_path)\n",
    "    splitted_idx = data.get_idx_split()\n",
    "    idx_train, idx_val, idx_test = (\n",
    "        splitted_idx[\"train\"],\n",
    "        splitted_idx[\"valid\"],\n",
    "        splitted_idx[\"test\"],\n",
    "    )\n",
    "\n",
    "    g, labels = data[0]\n",
    "    labels = labels.squeeze()\n",
    "\n",
    "    # Turn the graph to undirected\n",
    "    if dataset == \"ogbn-arxiv\":\n",
    "        srcs, dsts = g.all_edges()\n",
    "        g.add_edges(dsts, srcs)\n",
    "        g = g.remove_self_loop().add_self_loop()\n",
    "\n",
    "    return g, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def load_cpf_data(dataset, dataset_path, seed, labelrate_train, labelrate_val):\n",
    "    data_path = Path.cwd().joinpath(dataset_path, f\"{dataset}.npz\")\n",
    "    if os.path.isfile(data_path):\n",
    "        data = load_npz_to_sparse_graph(data_path)\n",
    "    else:\n",
    "        raise ValueError(f\"{data_path} doesn't exist.\")\n",
    "\n",
    "    # remove self loop and extract the largest CC\n",
    "    data = data.standardize()\n",
    "    adj, features, labels = data.unpack()\n",
    "\n",
    "    labels = binarize_labels(labels)\n",
    "\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    idx_train, idx_val, idx_test = get_train_val_test_split(\n",
    "        random_state, labels, labelrate_train, labelrate_val\n",
    "    )\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labels.argmax(axis=1))\n",
    "\n",
    "    adj = normalize_adj(adj)\n",
    "    adj_sp = adj.tocoo()\n",
    "    g = dgl.graph((adj_sp.row, adj_sp.col))\n",
    "    g.ndata[\"feat\"] = features\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    return g, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def load_nonhom_data(dataset, dataset_path, split_idx):\n",
    "    data_path = Path.cwd().joinpath(dataset_path, f\"{dataset}.mat\")\n",
    "    data_split_path = Path.cwd().joinpath(\n",
    "        dataset_path, \"splits\", f\"{dataset}-splits.npy\"\n",
    "    )\n",
    "\n",
    "    if dataset == \"pokec\":\n",
    "        g, features, labels = load_pokec_mat(data_path)\n",
    "    elif dataset == \"penn94\":\n",
    "        g, features, labels = load_penn94_mat(data_path)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataname\")\n",
    "\n",
    "    g = g.remove_self_loop().add_self_loop()\n",
    "    g.ndata[\"feat\"] = features\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    splitted_idx = load_fixed_splits(dataset, data_split_path, split_idx)\n",
    "    idx_train, idx_val, idx_test = (\n",
    "        splitted_idx[\"train\"],\n",
    "        splitted_idx[\"valid\"],\n",
    "        splitted_idx[\"test\"],\n",
    "    )\n",
    "    return g, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def load_bgnn_data(dataset, dataset_path, split_idx):\n",
    "    data_path = Path.cwd().joinpath(dataset_path, f\"{dataset}\")\n",
    "\n",
    "    g, X, y, cat_features, masks = read_input(data_path)\n",
    "    train_mask, val_mask, test_mask = (\n",
    "        masks[str(split_idx)][\"train\"],\n",
    "        masks[str(split_idx)][\"val\"],\n",
    "        masks[str(split_idx)][\"test\"],\n",
    "    )\n",
    "\n",
    "    encoded_X = X.copy()\n",
    "    if cat_features is not None and len(cat_features):\n",
    "        encoded_X = encode_cat_features(\n",
    "            encoded_X, y, cat_features, train_mask, val_mask, test_mask\n",
    "        )\n",
    "    encoded_X = normalize_features(encoded_X, train_mask, val_mask, test_mask)\n",
    "    encoded_X = replace_na(encoded_X, train_mask)\n",
    "    features, labels = pandas_to_torch(encoded_X, y)\n",
    "\n",
    "    g = g.remove_self_loop().add_self_loop()\n",
    "    g.ndata[\"feat\"] = features\n",
    "    labels = labels.long()\n",
    "\n",
    "    idx_train = torch.LongTensor(train_mask)\n",
    "    idx_val = torch.LongTensor(val_mask)\n",
    "    idx_test = torch.LongTensor(test_mask)\n",
    "    return g, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def load_out_t(out_t_dir):\n",
    "    return torch.from_numpy(np.load(out_t_dir.joinpath(\"out.npz\"))[\"arr_0\"])\n",
    "\n",
    "\n",
    "\"\"\" For NonHom\"\"\"\n",
    "dataset_drive_url = {\"pokec\": \"1dNs5E7BrWJbgcHeQ_zuy5Ozp2tRCWG0y\"}\n",
    "splits_drive_url = {\"pokec\": \"1ZhpAiyTNc0cE_hhgyiqxnkKREHK7MK-_\"}\n",
    "\n",
    "\n",
    "def load_penn94_mat(data_path):\n",
    "    mat = scipy.io.loadmat(data_path)\n",
    "    A = mat[\"A\"]\n",
    "    metadata = mat[\"local_info\"]\n",
    "\n",
    "    edge_index = torch.tensor(A.nonzero(), dtype=torch.long)\n",
    "    metadata = metadata.astype(np.int)\n",
    "\n",
    "    # make features into one-hot encodings\n",
    "    feature_vals = np.hstack((np.expand_dims(metadata[:, 0], 1), metadata[:, 2:]))\n",
    "    features = np.empty((A.shape[0], 0))\n",
    "    for col in range(feature_vals.shape[1]):\n",
    "        feat_col = feature_vals[:, col]\n",
    "        feat_onehot = label_binarize(feat_col, classes=np.unique(feat_col))\n",
    "        features = np.hstack((features, feat_onehot))\n",
    "\n",
    "    g = dgl.graph((edge_index[0], edge_index[1]))\n",
    "    g = dgl.to_bidirected(g)\n",
    "\n",
    "    features = torch.tensor(features, dtype=torch.float)\n",
    "    labels = torch.tensor(metadata[:, 1] - 1)  # gender label, -1 means unlabeled\n",
    "    return g, features, labels\n",
    "\n",
    "\n",
    "def load_pokec_mat(data_path):\n",
    "    if not path.exists(data_path):\n",
    "        gdd.download_file_from_google_drive(\n",
    "            file_id=dataset_drive_url[\"pokec\"], dest_path=data_path, showsize=True\n",
    "        )\n",
    "\n",
    "    fulldata = scipy.io.loadmat(data_path)\n",
    "    edge_index = torch.tensor(fulldata[\"edge_index\"], dtype=torch.long)\n",
    "    g = dgl.graph((edge_index[0], edge_index[1]))\n",
    "    g = dgl.to_bidirected(g)\n",
    "\n",
    "    features = torch.tensor(fulldata[\"node_feat\"]).float()\n",
    "    labels = fulldata[\"label\"].flatten()\n",
    "    return g, features, labels\n",
    "\n",
    "\n",
    "class NCDataset(object):\n",
    "    def __init__(self, name, root):\n",
    "        \"\"\"\n",
    "        based off of ogb NodePropPredDataset\n",
    "        https://github.com/snap-stanford/ogb/blob/master/ogb/nodeproppred/dataset.py\n",
    "        Gives torch tensors instead of numpy arrays\n",
    "            - name (str): name of the dataset\n",
    "            - root (str): root directory to store the dataset folder\n",
    "            - meta_dict: dictionary that stores all the meta-information about data. Default is None,\n",
    "                    but when something is passed, it uses its information. Useful for debugging for external contributers.\n",
    "\n",
    "        Usage after construction:\n",
    "\n",
    "        split_idx = dataset.get_idx_split()\n",
    "        train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "        graph, label = dataset[0]\n",
    "\n",
    "        Where the graph is a dictionary of the following form:\n",
    "        dataset.graph = {'edge_index': edge_index,\n",
    "                         'edge_feat': None,\n",
    "                         'node_feat': node_feat,\n",
    "                         'num_nodes': num_nodes}\n",
    "        For additional documentation, see OGB Library-Agnostic Loader https://ogb.stanford.edu/docs/nodeprop/\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.name = name  # original name, e.g., ogbn-proteins\n",
    "        self.graph = {}\n",
    "        self.label = None\n",
    "\n",
    "    def get_idx_split(self, split_type=\"random\", train_prop=0.5, valid_prop=0.25):\n",
    "        \"\"\"\n",
    "        train_prop: The proportion of dataset for train split. Between 0 and 1.\n",
    "        valid_prop: The proportion of dataset for validation split. Between 0 and 1.\n",
    "        \"\"\"\n",
    "\n",
    "        if split_type == \"random\":\n",
    "            ignore_negative = False if self.name == \"ogbn-proteins\" else True\n",
    "            train_idx, valid_idx, test_idx = rand_train_test_idx(\n",
    "                self.label,\n",
    "                train_prop=train_prop,\n",
    "                valid_prop=valid_prop,\n",
    "                ignore_negative=ignore_negative,\n",
    "            )\n",
    "            split_idx = {\"train\": train_idx, \"valid\": valid_idx, \"test\": test_idx}\n",
    "        return split_idx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx == 0, \"This dataset has only one graph\"\n",
    "        return self.graph, self.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}({})\".format(self.__class__.__name__, len(self))\n",
    "\n",
    "\n",
    "def load_fixed_splits(dataset, data_split_path=\"\", split_idx=0):\n",
    "    if not os.path.exists(data_split_path):\n",
    "        assert dataset in splits_drive_url.keys()\n",
    "        gdd.download_file_from_google_drive(\n",
    "            file_id=splits_drive_url[dataset], dest_path=data_split_path, showsize=True\n",
    "        )\n",
    "\n",
    "    splits_lst = np.load(data_split_path, allow_pickle=True)\n",
    "    splits = splits_lst[split_idx]\n",
    "\n",
    "    for key in splits:\n",
    "        if not torch.is_tensor(splits[key]):\n",
    "            splits[key] = torch.as_tensor(splits[key])\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "\"\"\"For BGNN \"\"\"\n",
    "\n",
    "\n",
    "def pandas_to_torch(*args):\n",
    "    return [torch.from_numpy(arg.to_numpy(copy=True)).float().squeeze() for arg in args]\n",
    "\n",
    "\n",
    "def read_input(input_folder):\n",
    "    X = pd.read_csv(f\"{input_folder}/X.csv\")\n",
    "    y = pd.read_csv(f\"{input_folder}/y.csv\")\n",
    "\n",
    "    categorical_columns = []\n",
    "    if os.path.exists(f\"{input_folder}/cat_features.txt\"):\n",
    "        with open(f\"{input_folder}/cat_features.txt\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    categorical_columns.append(line.strip())\n",
    "\n",
    "    cat_features = None\n",
    "    if categorical_columns:\n",
    "        columns = X.columns\n",
    "        cat_features = np.where(columns.isin(categorical_columns))[0]\n",
    "\n",
    "        for col in list(columns[cat_features]):\n",
    "            X[col] = X[col].astype(str)\n",
    "\n",
    "    gs, _ = load_graphs(f\"{input_folder}/graph.dgl\")\n",
    "    graph = gs[0]\n",
    "\n",
    "    with open(f\"{input_folder}/masks.json\") as f:\n",
    "        masks = json.load(f)\n",
    "\n",
    "    return graph, X, y, cat_features, masks\n",
    "\n",
    "\n",
    "def normalize_features(X, train_mask, val_mask, test_mask):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    A = X.to_numpy(copy=True)\n",
    "    A[train_mask] = min_max_scaler.fit_transform(A[train_mask])\n",
    "    A[val_mask + test_mask] = min_max_scaler.transform(A[val_mask + test_mask])\n",
    "    return pd.DataFrame(A, columns=X.columns).astype(float)\n",
    "\n",
    "\n",
    "def replace_na(X, train_mask):\n",
    "    if X.isna().any().any():\n",
    "        return X.fillna(X.iloc[train_mask].min() - 1)\n",
    "    return X\n",
    "\n",
    "\n",
    "def encode_cat_features(X, y, cat_features, train_mask, val_mask, test_mask):\n",
    "    enc = CatBoostEncoder()\n",
    "    A = X.to_numpy(copy=True)\n",
    "    b = y.to_numpy(copy=True)\n",
    "    A[np.ix_(train_mask, cat_features)] = enc.fit_transform(\n",
    "        A[np.ix_(train_mask, cat_features)], b[train_mask]\n",
    "    )\n",
    "    A[np.ix_(val_mask + test_mask, cat_features)] = enc.transform(\n",
    "        A[np.ix_(val_mask + test_mask, cat_features)]\n",
    "    )\n",
    "    A = A.astype(float)\n",
    "    return pd.DataFrame(A, columns=X.columns)\n",
    "\n",
    "\n",
    "\"\"\" For CPF\"\"\"\n",
    "\n",
    "\n",
    "class SparseGraph:\n",
    "    \"\"\"Attributed labeled graph stored in sparse matrix form.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adj_matrix,\n",
    "        attr_matrix=None,\n",
    "        labels=None,\n",
    "        node_names=None,\n",
    "        attr_names=None,\n",
    "        class_names=None,\n",
    "        metadata=None,\n",
    "    ):\n",
    "        \"\"\"Create an attributed graph.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adj_matrix : sp.csr_matrix, shape [num_nodes, num_nodes]\n",
    "            Adjacency matrix in CSR format.\n",
    "        attr_matrix : sp.csr_matrix or np.ndarray, shape [num_nodes, num_attr], optional\n",
    "            Attribute matrix in CSR or numpy format.\n",
    "        labels : np.ndarray, shape [num_nodes], optional\n",
    "            Array, where each entry represents respective node's label(s).\n",
    "        node_names : np.ndarray, shape [num_nodes], optional\n",
    "            Names of nodes (as strings).\n",
    "        attr_names : np.ndarray, shape [num_attr]\n",
    "            Names of the attributes (as strings).\n",
    "        class_names : np.ndarray, shape [num_classes], optional\n",
    "            Names of the class labels (as strings).\n",
    "        metadata : object\n",
    "            Additional metadata such as text.\n",
    "\n",
    "        \"\"\"\n",
    "        # Make sure that the dimensions of matrices / arrays all agree\n",
    "        if sp.isspmatrix(adj_matrix):\n",
    "            adj_matrix = adj_matrix.tocsr().astype(np.float32)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Adjacency matrix must be in sparse format (got {0} instead)\".format(\n",
    "                    type(adj_matrix)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "            raise ValueError(\"Dimensions of the adjacency matrix don't agree\")\n",
    "\n",
    "        if attr_matrix is not None:\n",
    "            if sp.isspmatrix(attr_matrix):\n",
    "                attr_matrix = attr_matrix.tocsr().astype(np.float32)\n",
    "            elif isinstance(attr_matrix, np.ndarray):\n",
    "                attr_matrix = attr_matrix.astype(np.float32)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Attribute matrix must be a sp.spmatrix or a np.ndarray (got {0} instead)\".format(\n",
    "                        type(attr_matrix)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if attr_matrix.shape[0] != adj_matrix.shape[0]:\n",
    "                raise ValueError(\n",
    "                    \"Dimensions of the adjacency and attribute matrices don't agree\"\n",
    "                )\n",
    "\n",
    "        if labels is not None:\n",
    "            if labels.shape[0] != adj_matrix.shape[0]:\n",
    "                raise ValueError(\n",
    "                    \"Dimensions of the adjacency matrix and the label vector don't agree\"\n",
    "                )\n",
    "\n",
    "        if node_names is not None:\n",
    "            if len(node_names) != adj_matrix.shape[0]:\n",
    "                raise ValueError(\n",
    "                    \"Dimensions of the adjacency matrix and the node names don't agree\"\n",
    "                )\n",
    "\n",
    "        if attr_names is not None:\n",
    "            if len(attr_names) != attr_matrix.shape[1]:\n",
    "                raise ValueError(\n",
    "                    \"Dimensions of the attribute matrix and the attribute names don't agree\"\n",
    "                )\n",
    "\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.attr_matrix = attr_matrix\n",
    "        self.labels = labels\n",
    "        self.node_names = node_names\n",
    "        self.attr_names = attr_names\n",
    "        self.class_names = class_names\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def num_nodes(self):\n",
    "        \"\"\"Get the number of nodes in the graph.\"\"\"\n",
    "        return self.adj_matrix.shape[0]\n",
    "\n",
    "    def num_edges(self):\n",
    "        \"\"\"Get the number of edges in the graph.\n",
    "\n",
    "        For undirected graphs, (i, j) and (j, i) are counted as single edge.\n",
    "        \"\"\"\n",
    "        if self.is_directed():\n",
    "            return int(self.adj_matrix.nnz)\n",
    "        else:\n",
    "            return int(self.adj_matrix.nnz / 2)\n",
    "\n",
    "    def get_neighbors(self, idx):\n",
    "        \"\"\"Get the indices of neighbors of a given node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the node whose neighbors are of interest.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.adj_matrix[idx].indices\n",
    "\n",
    "    def is_directed(self):\n",
    "        \"\"\"Check if the graph is directed (adjacency matrix is not symmetric).\"\"\"\n",
    "        return (self.adj_matrix != self.adj_matrix.T).sum() != 0\n",
    "\n",
    "    def to_undirected(self):\n",
    "        \"\"\"Convert to an undirected graph (make adjacency matrix symmetric).\"\"\"\n",
    "        if self.is_weighted():\n",
    "            raise ValueError(\"Convert to unweighted graph first.\")\n",
    "        else:\n",
    "            self.adj_matrix = self.adj_matrix + self.adj_matrix.T\n",
    "            self.adj_matrix[self.adj_matrix != 0] = 1\n",
    "        return self\n",
    "\n",
    "    def is_weighted(self):\n",
    "        \"\"\"Check if the graph is weighted (edge weights other than 1).\"\"\"\n",
    "        return np.any(np.unique(self.adj_matrix[self.adj_matrix != 0].A1) != 1)\n",
    "\n",
    "    def to_unweighted(self):\n",
    "        \"\"\"Convert to an unweighted graph (set all edge weights to 1).\"\"\"\n",
    "        self.adj_matrix.data = np.ones_like(self.adj_matrix.data)\n",
    "        return self\n",
    "\n",
    "    # Quality of life (shortcuts)\n",
    "    def standardize(self):\n",
    "        \"\"\"Select the LCC of the unweighted/undirected/no-self-loop graph.\n",
    "\n",
    "        All changes are done inplace.\n",
    "\n",
    "        \"\"\"\n",
    "        G = self.to_unweighted().to_undirected()\n",
    "        G.adj_matrix = eliminate_self_loops_adj(G.adj_matrix)\n",
    "        G = largest_connected_components(G, 1)\n",
    "        return G\n",
    "\n",
    "    def unpack(self):\n",
    "        \"\"\"Return the (A, X, z) triplet.\"\"\"\n",
    "        return self.adj_matrix, self.attr_matrix, self.labels\n",
    "\n",
    "\n",
    "def load_npz_to_sparse_graph(file_name):\n",
    "    \"\"\"Load a SparseGraph from a Numpy binary file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        Name of the file to load.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sparse_graph : SparseGraph\n",
    "        Graph in sparse matrix format.\n",
    "\n",
    "    \"\"\"\n",
    "    with np.load(file_name, allow_pickle=True) as loader:\n",
    "        loader = dict(loader)\n",
    "        adj_matrix = sp.csr_matrix(\n",
    "            (loader[\"adj_data\"], loader[\"adj_indices\"], loader[\"adj_indptr\"]),\n",
    "            shape=loader[\"adj_shape\"],\n",
    "        )\n",
    "\n",
    "        if \"attr_data\" in loader:\n",
    "            # Attributes are stored as a sparse CSR matrix\n",
    "            attr_matrix = sp.csr_matrix(\n",
    "                (loader[\"attr_data\"], loader[\"attr_indices\"], loader[\"attr_indptr\"]),\n",
    "                shape=loader[\"attr_shape\"],\n",
    "            )\n",
    "        elif \"attr_matrix\" in loader:\n",
    "            # Attributes are stored as a (dense) np.ndarray\n",
    "            attr_matrix = loader[\"attr_matrix\"]\n",
    "        else:\n",
    "            attr_matrix = None\n",
    "\n",
    "        if \"labels_data\" in loader:\n",
    "            # Labels are stored as a CSR matrix\n",
    "            labels = sp.csr_matrix(\n",
    "                (\n",
    "                    loader[\"labels_data\"],\n",
    "                    loader[\"labels_indices\"],\n",
    "                    loader[\"labels_indptr\"],\n",
    "                ),\n",
    "                shape=loader[\"labels_shape\"],\n",
    "            )\n",
    "        elif \"labels\" in loader:\n",
    "            # Labels are stored as a numpy array\n",
    "            labels = loader[\"labels\"]\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        node_names = loader.get(\"node_names\")\n",
    "        attr_names = loader.get(\"attr_names\")\n",
    "        class_names = loader.get(\"class_names\")\n",
    "        metadata = loader.get(\"metadata\")\n",
    "\n",
    "    return SparseGraph(\n",
    "        adj_matrix, attr_matrix, labels, node_names, attr_names, class_names, metadata\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_per_class(\n",
    "    random_state, labels, num_examples_per_class, forbidden_indices=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Used in get_train_val_test_split, when we try to get a fixed number of examples per class\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples, num_classes = labels.shape\n",
    "    sample_indices_per_class = {index: [] for index in range(num_classes)}\n",
    "\n",
    "    # get indices sorted by class\n",
    "    for class_index in range(num_classes):\n",
    "        for sample_index in range(num_samples):\n",
    "            if labels[sample_index, class_index] > 0.0:\n",
    "                if forbidden_indices is None or sample_index not in forbidden_indices:\n",
    "                    sample_indices_per_class[class_index].append(sample_index)\n",
    "\n",
    "    # get specified number of indices for each class\n",
    "    return np.concatenate(\n",
    "        [\n",
    "            random_state.choice(\n",
    "                sample_indices_per_class[class_index],\n",
    "                num_examples_per_class,\n",
    "                replace=False,\n",
    "            )\n",
    "            for class_index in range(len(sample_indices_per_class))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_train_val_test_split(\n",
    "    random_state,\n",
    "    labels,\n",
    "    train_examples_per_class=None,\n",
    "    val_examples_per_class=None,\n",
    "    test_examples_per_class=None,\n",
    "    train_size=None,\n",
    "    val_size=None,\n",
    "    test_size=None,\n",
    "):\n",
    "\n",
    "    num_samples, num_classes = labels.shape\n",
    "    remaining_indices = list(range(num_samples))\n",
    "    if train_examples_per_class is not None:\n",
    "        train_indices = sample_per_class(random_state, labels, train_examples_per_class)\n",
    "    else:\n",
    "        # select train examples with no respect to class distribution\n",
    "        train_indices = random_state.choice(\n",
    "            remaining_indices, train_size, replace=False\n",
    "        )\n",
    "\n",
    "    if val_examples_per_class is not None:\n",
    "        val_indices = sample_per_class(\n",
    "            random_state,\n",
    "            labels,\n",
    "            val_examples_per_class,\n",
    "            forbidden_indices=train_indices,\n",
    "        )\n",
    "    else:\n",
    "        remaining_indices = np.setdiff1d(remaining_indices, train_indices)\n",
    "        val_indices = random_state.choice(remaining_indices, val_size, replace=False)\n",
    "\n",
    "    forbidden_indices = np.concatenate((train_indices, val_indices))\n",
    "    if test_examples_per_class is not None:\n",
    "        test_indices = sample_per_class(\n",
    "            random_state,\n",
    "            labels,\n",
    "            test_examples_per_class,\n",
    "            forbidden_indices=forbidden_indices,\n",
    "        )\n",
    "    elif test_size is not None:\n",
    "        remaining_indices = np.setdiff1d(remaining_indices, forbidden_indices)\n",
    "        test_indices = random_state.choice(remaining_indices, test_size, replace=False)\n",
    "    else:\n",
    "        test_indices = np.setdiff1d(remaining_indices, forbidden_indices)\n",
    "\n",
    "    # assert that there are no duplicates in sets\n",
    "    assert len(set(train_indices)) == len(train_indices)\n",
    "    assert len(set(val_indices)) == len(val_indices)\n",
    "    assert len(set(test_indices)) == len(test_indices)\n",
    "    # assert sets are mutually exclusive\n",
    "    assert len(set(train_indices) - set(val_indices)) == len(set(train_indices))\n",
    "    assert len(set(train_indices) - set(test_indices)) == len(set(train_indices))\n",
    "    assert len(set(val_indices) - set(test_indices)) == len(set(val_indices))\n",
    "    if test_size is None and test_examples_per_class is None:\n",
    "        # all indices must be part of the split\n",
    "        assert (\n",
    "            len(np.concatenate((train_indices, val_indices, test_indices)))\n",
    "            == num_samples\n",
    "        )\n",
    "\n",
    "    if train_examples_per_class is not None:\n",
    "        train_labels = labels[train_indices, :]\n",
    "        train_sum = np.sum(train_labels, axis=0)\n",
    "        # assert all classes have equal cardinality\n",
    "        assert np.unique(train_sum).size == 1\n",
    "\n",
    "    if val_examples_per_class is not None:\n",
    "        val_labels = labels[val_indices, :]\n",
    "        val_sum = np.sum(val_labels, axis=0)\n",
    "        # assert all classes have equal cardinality\n",
    "        assert np.unique(val_sum).size == 1\n",
    "\n",
    "    if test_examples_per_class is not None:\n",
    "        test_labels = labels[test_indices, :]\n",
    "        test_sum = np.sum(test_labels, axis=0)\n",
    "        # assert all classes have equal cardinality\n",
    "        assert np.unique(test_sum).size == 1\n",
    "\n",
    "    return train_indices, val_indices, test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_student.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from models import Model\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch DGL implementation\")\n",
    "    parser.add_argument(\"--device\", type=int, default=-1, help=\"CUDA device, -1 means CPU\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed\")\n",
    "    parser.add_argument(\n",
    "        \"--log_level\",\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help=\"Logger levels for run {10: DEBUG, 20: INFO, 30: WARNING}\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--console_log\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Set to True to display log info in console\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_path\", type=str, default=\"outputs\", help=\"Path to save outputs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_exp\", type=int, default=1, help=\"Repeat how many experiments\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--exp_setting\",\n",
    "        type=str,\n",
    "        default=\"tran\",\n",
    "        help=\"Experiment setting, one of [tran, ind]\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_interval\", type=int, default=1, help=\"Evaluate once per how many epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_results\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Set to True to save the loss curves, trained model, and min-cut loss for the transductive setting\",\n",
    "    )\n",
    "\n",
    "    \"\"\"Dataset\"\"\"\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"cora\", help=\"Dataset\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"./data\", help=\"Path to data\")\n",
    "    parser.add_argument(\n",
    "        \"--labelrate_train\",\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help=\"How many labeled data per class as train set\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--labelrate_val\",\n",
    "        type=int,\n",
    "        default=30,\n",
    "        help=\"How many labeled data per class in valid set\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--split_idx\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"For Non-Homo datasets only, one of [0,1,2,3,4]\",\n",
    "    )\n",
    "\n",
    "    \"\"\"Model\"\"\"\n",
    "    parser.add_argument(\n",
    "        \"--model_config_path\",\n",
    "        type=str,\n",
    "        default=\"./train.conf.yaml\",\n",
    "        help=\"Path to model configeration\",\n",
    "    )\n",
    "    parser.add_argument(\"--teacher\", type=str, default=\"SAGE\", help=\"Teacher model\")\n",
    "    parser.add_argument(\"--student\", type=str, default=\"MLP\", help=\"Student model\")\n",
    "    parser.add_argument(\n",
    "        \"--num_layers\", type=int, default=2, help=\"Student model number of layers\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_dim\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        help=\"Student model hidden layer dimensions\",\n",
    "    )\n",
    "    parser.add_argument(\"--dropout_ratio\", type=float, default=0)\n",
    "    parser.add_argument(\n",
    "        \"--norm_type\", type=str, default=\"none\", help=\"One of [none, batch, layer]\"\n",
    "    )\n",
    "\n",
    "    \"\"\"SAGE Specific\"\"\"\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=512)\n",
    "    parser.add_argument(\n",
    "        \"--fan_out\",\n",
    "        type=str,\n",
    "        default=\"5,5\",\n",
    "        help=\"Number of samples for each layer in SAGE. Length = num_layers\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_workers\", type=int, default=0, help=\"Number of workers for sampler\"\n",
    "    )\n",
    "\n",
    "    \"\"\"Optimization\"\"\"\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.0005)\n",
    "    parser.add_argument(\n",
    "        \"--max_epoch\", type=int, default=500, help=\"Evaluate once per how many epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--patience\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"Early stop is the score on validation set does not improve for how many epochs\",\n",
    "    )\n",
    "\n",
    "    \"\"\"Ablation\"\"\"\n",
    "    parser.add_argument(\n",
    "        \"--feature_noise\",\n",
    "        type=float,\n",
    "        default=0,\n",
    "        help=\"add white noise to features for analysis, value in [0, 1] for noise level\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--split_rate\",\n",
    "        type=float,\n",
    "        default=0.2,\n",
    "        help=\"Rate for graph split, see comment of graph_split for more details\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--compute_min_cut\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Set to True to compute and store the min-cut loss\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--feature_aug_k\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Augment node futures by aggregating feature_aug_k-hop neighbor features\",\n",
    "    )\n",
    "\n",
    "    \"\"\"Distiall\"\"\"\n",
    "    parser.add_argument(\n",
    "        \"--lamb\",\n",
    "        type=float,\n",
    "        default=0,\n",
    "        help=\"Parameter balances loss from hard labels and teacher outputs, take values in [0, 1]\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_t_path\", type=str, default=\"outputs\", help=\"Path to load teacher outputs\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def run(args):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    score_lst: a list of evaluation results on test set.\n",
    "    len(score_lst) = 1 for the transductive setting.\n",
    "    len(score_lst) = 2 for the inductive/production setting.\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" Set seed, device, and logger \"\"\"\n",
    "    set_seed(args.seed)\n",
    "    if torch.cuda.is_available() and args.device >= 0:\n",
    "        device = torch.device(\"cuda:\" + str(args.device))\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    if args.feature_noise != 0:\n",
    "        args.output_path = Path.cwd().joinpath(\n",
    "            args.output_path, \"noisy_features\", f\"noise_{args.feature_noise}\"\n",
    "        )\n",
    "        # Teacher is assumed to be trained on the same noisy features as well.\n",
    "        args.out_t_path = args.output_path\n",
    "\n",
    "    if args.feature_aug_k > 0:\n",
    "        args.output_path = Path.cwd().joinpath(\n",
    "            args.output_path, \"aug_features\", f\"aug_hop_{args.feature_aug_k}\"\n",
    "        )\n",
    "        # NOTE: Teacher may or may not have augmented features, specify args.out_t_path explicitly.\n",
    "        # args.out_t_path =\n",
    "        args.student = f\"GA{args.feature_aug_k}{args.student}\"\n",
    "\n",
    "    if args.exp_setting == \"tran\":\n",
    "        output_dir = Path.cwd().joinpath(\n",
    "            args.output_path,\n",
    "            \"transductive\",\n",
    "            args.dataset,\n",
    "            f\"{args.teacher}_{args.student}\",\n",
    "            f\"seed_{args.seed}\",\n",
    "        )\n",
    "        out_t_dir = Path.cwd().joinpath(\n",
    "            args.out_t_path,\n",
    "            \"transductive\",\n",
    "            args.dataset,\n",
    "            args.teacher,\n",
    "            f\"seed_{args.seed}\",\n",
    "        )\n",
    "    elif args.exp_setting == \"ind\":\n",
    "        output_dir = Path.cwd().joinpath(\n",
    "            args.output_path,\n",
    "            \"inductive\",\n",
    "            f\"split_rate_{args.split_rate}\",\n",
    "            args.dataset,\n",
    "            f\"{args.teacher}_{args.student}\",\n",
    "            f\"seed_{args.seed}\",\n",
    "        )\n",
    "        out_t_dir = Path.cwd().joinpath(\n",
    "            args.out_t_path,\n",
    "            \"inductive\",\n",
    "            f\"split_rate_{args.split_rate}\",\n",
    "            args.dataset,\n",
    "            args.teacher,\n",
    "            f\"seed_{args.seed}\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown experiment setting! {args.exp_setting}\")\n",
    "    args.output_dir = output_dir\n",
    "\n",
    "    check_writable(output_dir, overwrite=False)\n",
    "    check_readable(out_t_dir)\n",
    "\n",
    "    logger = get_logger(output_dir.joinpath(\"log\"), args.console_log, args.log_level)\n",
    "    logger.info(f\"output_dir: {output_dir}\")\n",
    "    logger.info(f\"out_t_dir: {out_t_dir}\")\n",
    "\n",
    "    \"\"\" Load data and model config\"\"\"\n",
    "    g, labels, idx_train, idx_val, idx_test = load_data(\n",
    "        args.dataset,\n",
    "        args.data_path,\n",
    "        split_idx=args.split_idx,\n",
    "        seed=args.seed,\n",
    "        labelrate_train=args.labelrate_train,\n",
    "        labelrate_val=args.labelrate_val,\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Total {g.number_of_nodes()} nodes.\")\n",
    "    logger.info(f\"Total {g.number_of_edges()} edges.\")\n",
    "\n",
    "    feats = g.ndata[\"feat\"]\n",
    "    args.feat_dim = g.ndata[\"feat\"].shape[1]\n",
    "    args.label_dim = labels.int().max().item() + 1\n",
    "\n",
    "    if 0 < args.feature_noise <= 1:\n",
    "        feats = (\n",
    "            1 - args.feature_noise\n",
    "        ) * feats + args.feature_noise * torch.randn_like(feats)\n",
    "\n",
    "    \"\"\" Model config \"\"\"\n",
    "    conf = {}\n",
    "    if args.model_config_path is not None:\n",
    "        conf = get_training_config(\n",
    "            args.model_config_path, args.student, args.dataset\n",
    "        )  # Note: student config\n",
    "    conf = dict(args.__dict__, **conf)\n",
    "    conf[\"device\"] = device\n",
    "    logger.info(f\"conf: {conf}\")\n",
    "\n",
    "    \"\"\" Model init \"\"\"\n",
    "    model = Model(conf)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=conf[\"learning_rate\"], weight_decay=conf[\"weight_decay\"]\n",
    "    )\n",
    "    criterion_l = torch.nn.NLLLoss()\n",
    "    criterion_t = torch.nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "    evaluator = get_evaluator(conf[\"dataset\"])\n",
    "\n",
    "    \"\"\"Load teacher model output\"\"\"\n",
    "    out_t = load_out_t(out_t_dir)\n",
    "    logger.debug(\n",
    "        f\"teacher score on train data: {evaluator(out_t[idx_train], labels[idx_train])}\"\n",
    "    )\n",
    "    logger.debug(\n",
    "        f\"teacher score on val data: {evaluator(out_t[idx_val], labels[idx_val])}\"\n",
    "    )\n",
    "    logger.debug(\n",
    "        f\"teacher score on test data: {evaluator(out_t[idx_test], labels[idx_test])}\"\n",
    "    )\n",
    "\n",
    "    \"\"\"Data split and run\"\"\"\n",
    "    loss_and_score = []\n",
    "    if args.exp_setting == \"tran\":\n",
    "        idx_l = idx_train\n",
    "        idx_t = torch.cat([idx_train, idx_val, idx_test])\n",
    "        distill_indices = (idx_l, idx_t, idx_val, idx_test)\n",
    "\n",
    "        # propagate node feature\n",
    "        if args.feature_aug_k > 0:\n",
    "            feats = feature_prop(feats, g, args.feature_aug_k)\n",
    "\n",
    "        out, score_val, score_test = distill_run_transductive(\n",
    "            conf,\n",
    "            model,\n",
    "            feats,\n",
    "            labels,\n",
    "            out_t,\n",
    "            distill_indices,\n",
    "            criterion_l,\n",
    "            criterion_t,\n",
    "            evaluator,\n",
    "            optimizer,\n",
    "            logger,\n",
    "            loss_and_score,\n",
    "        )\n",
    "        score_lst = [score_test]\n",
    "\n",
    "    elif args.exp_setting == \"ind\":\n",
    "        # Create inductive split\n",
    "        obs_idx_train, obs_idx_val, obs_idx_test, idx_obs, idx_test_ind = graph_split(\n",
    "            idx_train, idx_val, idx_test, args.split_rate, args.seed\n",
    "        )\n",
    "        obs_idx_l = obs_idx_train\n",
    "        obs_idx_t = torch.cat([obs_idx_train, obs_idx_val, obs_idx_test])\n",
    "        distill_indices = (\n",
    "            obs_idx_l,\n",
    "            obs_idx_t,\n",
    "            obs_idx_val,\n",
    "            obs_idx_test,\n",
    "            idx_obs,\n",
    "            idx_test_ind,\n",
    "        )\n",
    "\n",
    "        # propagate node feature. The propagation for the observed graph only happens within the subgraph obs_g\n",
    "        if args.feature_aug_k > 0:\n",
    "            obs_g = g.subgraph(idx_obs)\n",
    "            obs_feats = feature_prop(feats[idx_obs], obs_g, args.feature_aug_k)\n",
    "            feats = feature_prop(feats, g, args.feature_aug_k)\n",
    "            feats[idx_obs] = obs_feats\n",
    "\n",
    "        out, score_val, score_test_tran, score_test_ind = distill_run_inductive(\n",
    "            conf,\n",
    "            model,\n",
    "            feats,\n",
    "            labels,\n",
    "            out_t,\n",
    "            distill_indices,\n",
    "            criterion_l,\n",
    "            criterion_t,\n",
    "            evaluator,\n",
    "            optimizer,\n",
    "            logger,\n",
    "            loss_and_score,\n",
    "        )\n",
    "        score_lst = [score_test_tran, score_test_ind]\n",
    "\n",
    "    logger.info(\n",
    "        f\"num_layers: {conf['num_layers']}. hidden_dim: {conf['hidden_dim']}. dropout_ratio: {conf['dropout_ratio']}\"\n",
    "    )\n",
    "    logger.info(f\"# params {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    \"\"\" Saving student outputs \"\"\"\n",
    "    out_np = out.detach().cpu().numpy()\n",
    "    np.savez(output_dir.joinpath(\"out\"), out_np)\n",
    "\n",
    "    \"\"\" Saving loss curve and model \"\"\"\n",
    "    if args.save_results:\n",
    "        # Loss curves\n",
    "        loss_and_score = np.array(loss_and_score)\n",
    "        np.savez(output_dir.joinpath(\"loss_and_score\"), loss_and_score)\n",
    "\n",
    "        # Model\n",
    "        torch.save(model.state_dict(), output_dir.joinpath(\"model.pth\"))\n",
    "\n",
    "    \"\"\" Saving min-cut loss\"\"\"\n",
    "    if args.exp_setting == \"tran\" and args.compute_min_cut:\n",
    "        min_cut = compute_min_cut_loss(g, out)\n",
    "        with open(output_dir.parent.joinpath(\"min_cut_loss\"), \"a+\") as f:\n",
    "            f.write(f\"{min_cut :.4f}\\n\")\n",
    "\n",
    "    return score_lst\n",
    "\n",
    "\n",
    "def repeat_run(args):\n",
    "    scores = []\n",
    "    for seed in range(args.num_exp):\n",
    "        args.seed = seed\n",
    "        scores.append(run(args))\n",
    "    scores_np = np.array(scores)\n",
    "    return scores_np.mean(axis=0), scores_np.std(axis=0)\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    if args.num_exp == 1:\n",
    "        score = run(args)\n",
    "        score_str = \"\".join([f\"{s : .4f}\\t\" for s in score])\n",
    "\n",
    "    elif args.num_exp > 1:\n",
    "        score_mean, score_std = repeat_run(args)\n",
    "        score_str = \"\".join(\n",
    "            [f\"{s : .4f}\\t\" for s in score_mean] + [f\"{s : .4f}\\t\" for s in score_std]\n",
    "        )\n",
    "\n",
    "    with open(args.output_dir.parent.joinpath(\"exp_results\"), \"a+\") as f:\n",
    "        f.write(f\"{score_str}\\n\")\n",
    "\n",
    "    # for collecting aggregated results\n",
    "    print(score_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--device DEVICE] [--seed SEED]\n",
      "                             [--log_level LOG_LEVEL] [--console_log]\n",
      "                             [--output_path OUTPUT_PATH] [--num_exp NUM_EXP]\n",
      "                             [--exp_setting EXP_SETTING]\n",
      "                             [--eval_interval EVAL_INTERVAL] [--save_results]\n",
      "                             [--dataset DATASET] [--data_path DATA_PATH]\n",
      "                             [--labelrate_train LABELRATE_TRAIN]\n",
      "                             [--labelrate_val LABELRATE_VAL]\n",
      "                             [--split_idx SPLIT_IDX]\n",
      "                             [--model_config_path MODEL_CONFIG_PATH]\n",
      "                             [--teacher TEACHER] [--student STUDENT]\n",
      "                             [--num_layers NUM_LAYERS]\n",
      "                             [--hidden_dim HIDDEN_DIM]\n",
      "                             [--dropout_ratio DROPOUT_RATIO]\n",
      "                             [--norm_type NORM_TYPE] [--batch_size BATCH_SIZE]\n",
      "                             [--fan_out FAN_OUT] [--num_workers NUM_WORKERS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--max_epoch MAX_EPOCH] [--patience PATIENCE]\n",
      "                             [--feature_noise FEATURE_NOISE]\n",
      "                             [--split_rate SPLIT_RATE] [--compute_min_cut]\n",
      "                             [--feature_aug_k FEATURE_AUG_K] [--lamb LAMB]\n",
      "                             [--out_t_path OUT_T_PATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/lxer/Library/Jupyter/runtime/kernel-ca45cf31-54f7-4c39-9a92-16e41e37a574.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
